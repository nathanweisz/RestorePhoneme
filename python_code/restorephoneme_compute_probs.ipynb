{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "#from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string #for later cleaning\n",
    "import textgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Language Model ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.no_grad at 0x7fcd1b601be0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading Language Model ...')\n",
    "\n",
    "# Loading TransformerXL language model\n",
    "#tokenizer_language_model = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "#language_model =  AutoModel.from_pretrained(\"bert-base-german-cased\")\n",
    "\n",
    "tokenizer_language_model = AutoTokenizer.from_pretrained(\"distilbert-base-german-cased\")\n",
    "language_model =  AutoModelForMaskedLM.from_pretrained(\"distilbert-base-german-cased\")\n",
    "\n",
    "\n",
    "#with io.capture_output() as captured:\n",
    "\n",
    "torch.no_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code provided by laura.aina@upf.edu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_distribution_over_next_word(language_model, context,\n",
    "                                                tokenizer_language_model):\n",
    "  '''\n",
    "  Given a string containing a linguistic context,\n",
    "  return the probability distribution over the vocabulary for the next word.\n",
    "  '''\n",
    "  # Encode context: transform words into indices (number identifier)\n",
    "  context_indices = tokenizer_language_model.encode(context)\n",
    "  # Find [MASK]\n",
    "  mask_id = tokenizer_language_model.encode(tokenizer_language_model.mask_token)[1]\n",
    "  index_target = context_indices.index(mask_id)\n",
    "  # List of indices in a vector\n",
    "  context = torch.tensor(context_indices).unsqueeze(0)\n",
    "  # Run model on context (vector of word indices) and get next-word scores over the vocabulary\n",
    "  # Vector of the size of the vocabulary containing a score per word\n",
    "  next_word_scores = language_model(context)[0].squeeze(0)[index_target]\n",
    "  # Trasform scores into probabilities through softmax function\n",
    "  # probability distribution over the words in the vocabulary\n",
    "  next_word_probability_distribution = F.softmax(next_word_scores, dim = -1)\n",
    "  return next_word_probability_distribution\n",
    "\n",
    "def get_top_next_words(language_model, context, tokenizer_language_model, n = 10, with_probabilities = False):\n",
    "  '''\n",
    "  Given a string containing a linguistic context,\n",
    "  return the n words with the highest probability.\n",
    "  If with_probabilities, also returns their associated probability scores.\n",
    "  '''\n",
    "  # Deploy model on context and get the probability distribution for the next word\n",
    "  # Vector of the size of the vocabulary containing a probability score per word\n",
    "  next_word_probability_distribution = get_probability_distribution_over_next_word(language_model,\n",
    "                                                                                   context, tokenizer_language_model)\n",
    "  # Sort words by probability score and get the top n words as candidates\n",
    "  sorted_word_candidates = torch.argsort(next_word_probability_distribution, descending = True)[:n]\n",
    "  # Decode list of candidates: from indices to word forms\n",
    "  top_word_candidates = [tokenizer_language_model.decode(torch.Tensor([w])).replace(' ', '') for w in sorted_word_candidates]\n",
    "  # If probabilities need to be output, match top words to their probabilities and return them together\n",
    "  if with_probabilities:\n",
    "    # Sort probability scores\n",
    "    sorted_probabilities = list(torch.sort(next_word_probability_distribution, descending = True)[0][:n])\n",
    "    # Pair words to probabilities and store them together\n",
    "    top_word_candidates_with_probabilities = []\n",
    "    for i in range(len(top_word_candidates)):\n",
    "      top_word_candidates_with_probabilities.append((top_word_candidates[i], float(sorted_probabilities[i])))\n",
    "    return top_word_candidates_with_probabilities\n",
    "  else:\n",
    "  # Else, return list of top words\n",
    "    return top_word_candidates\n",
    "\n",
    "def get_probability_of_word(language_model, context, tokenizer_language_model, word):\n",
    "    '''\n",
    "    Given two strings containing a linguistic context and a candidate word to complete it, respectively,\n",
    "    returns the probability of the word according to the language model.\n",
    "    '''\n",
    "    # Deploy model on context and get the probability distribution for the next word\n",
    "    # Vector of the size of the vocabulary containing a probability score per word\n",
    "    next_word_probability_distribution = get_probability_distribution_over_next_word(language_model,\n",
    "                                                                                   context, tokenizer_language_model)\n",
    "    # Encode word: transform into index (number identifier)\n",
    "    word = tokenizer_language_model.encode(word)\n",
    "    # Acess probability for that word\n",
    "    probability_score = float(next_word_probability_distribution[word][1])\n",
    "    return float(probability_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove punctuations from predicted words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(next_word_list):\n",
    "    '''\n",
    "    removes predicted punctuations from list\n",
    "    '''\n",
    "    next_word_list_DF=pd.DataFrame(next_word_list,\n",
    "                                  columns=['Word', 'Prob'])\n",
    "    \n",
    "    ind_punct = np.empty([len(next_word_list)])\n",
    "\n",
    "    for ii in range(len(next_word_list)):\n",
    "        ind_punct[ii]=int(next_word_list[ii][0] in string.punctuation)\n",
    "    \n",
    "    next_word_list_DF=next_word_list_DF.drop(list(np.nonzero(ind_punct)[0]))\n",
    "    \n",
    "    return next_word_list_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUFF FOR ENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def textgrid2trans(textgridfile):\n",
    "    tg = textgrid.TextGrid.fromFile(textgridfile)\n",
    "    transcr=[]\n",
    "\n",
    "    for ii in range(len(tg.tiers[0])):\n",
    "        transcr.append(tg.tiers[0].intervals[ii].mark)\n",
    "        \n",
    "    return transcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgridfile='/Users/b1019548/Documents/GitHub/Experiments/RestorePhoneme/listening_paradigm/Audio final version/Fabi/textgrids/1_1_m.TextGrid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(textgridfile, context_length = 5, preds4entropy_N = 5):\n",
    "    transcr = textgrid2trans(textgridfile)\n",
    "\n",
    "    nwords = len(transcr)\n",
    "\n",
    "    entropyPD = pd.DataFrame(columns=['Context', 'Entropy'])\n",
    "\n",
    "    for ii in range(context_length-1):\n",
    "        transcr.insert(0, '')\n",
    "    \n",
    "    ss = 0\n",
    "    ee = context_length\n",
    "\n",
    "    \n",
    "\n",
    "    while ee <= len(transcr):\n",
    "        context = ' '.join(transcr[ss:ee]) + ' [MASK]'\n",
    "        ss = ss+1\n",
    "        ee = ee+1\n",
    "    \n",
    "        number_of_candidates = 30 #@param {type:\"slider\", min:1, max:20, step:1}\n",
    "        # Specify if you want to also output their probabilities\n",
    "        with_probabilities = True #@param {type:\"boolean\"}\n",
    "\n",
    "        # Deploy language model on context and get the n words with highest probability\n",
    "        next_word_list = get_top_next_words(language_model, context, tokenizer_language_model,\n",
    "                                    n = number_of_candidates, with_probabilities = with_probabilities)\n",
    "    \n",
    "        next_word_list=remove_punctuations(next_word_list)\n",
    "    \n",
    "        entropyPD = entropyPD.append({'Context' : context, 'Entropy' : entropy(next_word_list['Prob'][:preds4entropy_N])},\n",
    "                                ignore_index=True)\n",
    "    \n",
    "    return entropyPD\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOP OVER FABI AND JULIANE FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/b1019548/Documents/GitHub/Experiments/RestorePhoneme/listening_paradigm/Audio final version/')\n",
    "folderNames = ['Fabi', 'Julie']\n",
    "outDir = '/Entropy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(folderNames)):\n",
    "\n",
    "    os.makedirs(folderNames[ii] + outDir, exist_ok = True)\n",
    "    AllTextGrids = os.listdir(folderNames[ii] + '/textgrids')\n",
    "    \n",
    "    for tt in range(len(AllTextGrids)):\n",
    "        entropyPD = compute_entropy(folderNames[ii] + '/textgrids/' + AllTextGrids[tt], context_length = 8)\n",
    "        entropyPD.to_csv(folderNames[ii] + outDir + AllTextGrids[tt].split('.')[0] + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUFF FOR SURPRISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wordprob(textgridfile, context_length = 5):\n",
    "    transcr = textgrid2trans(textgridfile)\n",
    "\n",
    "    nwords = len(transcr)\n",
    "\n",
    "\n",
    "    surprisePD = pd.DataFrame(columns=['Context', 'Word', 'Probability'])\n",
    "\n",
    "    for ii in range(context_length):\n",
    "        transcr.insert(0, '')\n",
    "    \n",
    "    ss = 0\n",
    "    ee = context_length\n",
    "\n",
    "    while ee < len(transcr):\n",
    "        context = ' '.join(transcr[ss:ee]) + ' [MASK]'\n",
    "    \n",
    "        # Deploy language model on context and get the n words with highest probability\n",
    "        next_word_prob = get_probability_of_word(language_model, context, tokenizer_language_model, transcr[ee])\n",
    "    \n",
    "        surprisePD = surprisePD.append({'Context' : context, 'Word' : transcr[ee], 'Probability' : next_word_prob},\n",
    "                                ignore_index=True)\n",
    "    \n",
    "        ss = ss+1\n",
    "        ee = ee+1\n",
    "\n",
    "    return surprisePD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOP OVER FABI AND JULIANE FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/b1019548/Documents/GitHub/Experiments/RestorePhoneme/listening_paradigm/Audio final version/')\n",
    "folderNames = ['Fabi', 'Julie']\n",
    "outDir = '/WordProb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(folderNames)):\n",
    "\n",
    "    os.makedirs(folderNames[ii] + outDir, exist_ok = True)\n",
    "    AllTextGrids = os.listdir(folderNames[ii] + '/textgrids')\n",
    "    \n",
    "    for tt in range(len(AllTextGrids)):\n",
    "        surprisePD = compute_wordprob(folderNames[ii] + '/textgrids/' + AllTextGrids[tt], context_length = 8)\n",
    "        surprisePD.to_csv(folderNames[ii] + outDir + AllTextGrids[tt].split('.')[0] + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Entropy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(entropyPD['Entropy'][100:150], marker='o', markerfacecolor='blue', markersize=12, linewidth=4)\n",
    "plt.ylabel('Entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Probability"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(surprisePD['Probability'][0:50], marker='o', markerfacecolor='blue', markersize=12, linewidth=4)\n",
    "plt.ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find tokens and remove them:\n",
    "https://github.com/huggingface/transformers/issues/4827"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
